{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e81be81-45c6-453e-909d-cd5e0a7a99e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uklfr/gottbert-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizerFast\n",
    "from transformers import LongformerModel, LongformerTokenizerFast\n",
    "\n",
    "roberta_model = RobertaModel.from_pretrained(\"uklfr/gottbert-base\")\n",
    "roberta_tokenizer = RobertaTokenizerFast.from_pretrained(\"uklfr/gottbert-base\")\n",
    "longformer_model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a11022-2224-40b7-92e3-d34327cd569c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('layer.0.attention.self.query.weight',\n",
       " tensor([[ 0.0709,  0.0058, -0.0958,  ...,  0.1009,  0.0916, -0.1061],\n",
       "         [-0.0454,  0.1986,  0.0717,  ...,  0.0694,  0.0555,  0.1310],\n",
       "         [ 0.0862,  0.0564, -0.0507,  ..., -0.0383, -0.0048,  0.1035],\n",
       "         ...,\n",
       "         [-0.1843,  0.0107, -0.0321,  ..., -0.0515,  0.1050, -0.1197],\n",
       "         [-0.2524,  0.0441,  0.0672,  ...,  0.0712, -0.1082,  0.0129],\n",
       "         [-0.0488, -0.0893,  0.1063,  ..., -0.1860,  0.0062, -0.0533]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(longformer_model.encoder.state_dict().items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "555fc0b6-5977-4308-9b18-ec813fa02a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('layer.0.attention.self.query.weight',\n",
       " tensor([[-0.0269,  0.0167,  0.1127,  ...,  0.0181, -0.0102,  0.0051],\n",
       "         [ 0.0810,  0.0226,  0.0244,  ..., -0.0358,  0.0325,  0.0302],\n",
       "         [ 0.0144,  0.0280,  0.0135,  ..., -0.0652,  0.0628, -0.0140],\n",
       "         ...,\n",
       "         [-0.0131,  0.0109,  0.0334,  ..., -0.0476,  0.0423,  0.0392],\n",
       "         [ 0.0170,  0.0442, -0.0579,  ..., -0.0340, -0.0264, -0.0402],\n",
       "         [ 0.0092, -0.0088, -0.0338,  ...,  0.0180,  0.0569,  0.0584]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(roberta_model.encoder.state_dict().items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f828edec-b712-450f-b789-4411121c3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "def convert_roberta_to_longformer(\n",
    "    roberta_model,\n",
    "    roberta_tokenizer,\n",
    "    longformer_model,\n",
    "    longformer_max_length: int = None):\n",
    "    \n",
    "    if longformer_max_length is None:\n",
    "        longformer_max_length = longformer_model.config.max_position_embeddings + 1\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###############################\n",
    "    # Create longformer tokenizer #\n",
    "    ###############################\n",
    "    \n",
    "    # Longformer tokenizers are Roberta tokenizers.\n",
    "    # But to follow the conventions \n",
    "    # and to avoid confusion we create a \n",
    "    # longformer tokenizer class with the state of\n",
    "    # the original tokenizer.\n",
    "    with TemporaryDirectory() as temp_dir:\n",
    "        roberta_tokenizer.save_pretrained(temp_dir)\n",
    "        longformer_tokenizer = LongformerTokenizerFast.from_pretrained(temp_dir)\n",
    "    longformer_tokenizer.model_max_length = longformer_max_length\n",
    "    longformer_tokenizer.init_kwargs['model_max_length'] = longformer_max_length\n",
    "    \n",
    "    \n",
    "    \n",
    "    ######################\n",
    "    # Copy model weights #\n",
    "    ######################\n",
    "    \n",
    "    # We only copy the encoder weights and resize the embeddings.\n",
    "    # Pooler weights are kept untouched.\n",
    "    \n",
    "    #---------#\n",
    "    # Encoder #\n",
    "    #---------#\n",
    "    roberta_parameters = roberta_model.encoder.state_dict()\n",
    "    longformer_parameters = longformer_model.encoder.state_dict()\n",
    "    \n",
    "    # Load all compatible keys directly and obtain missing keys to handle later\n",
    "    errors = longformer_model.encoder.load_state_dict(roberta_parameters, strict=False)\n",
    "    assert not errors.unexpected_keys, \"Found unexpected keys\"\n",
    "    missing_keys = errors.missing_keys\n",
    "    \n",
    "    # We expect, the keys to be the weights of the global attention modules and\n",
    "    # reuse roberta's normal attention weights for those modules.\n",
    "    for longformer_key in missing_keys:\n",
    "        # Resolve layer properties\n",
    "        prefix, layer_idx, layer_class, layer_type, target, params = longformer_key.split(\".\")\n",
    "        assert layer_class == \"attention\" or target.endswith(\"global\"), f\"Unexcpected parameters {longformer_key}.\"\n",
    "        # Copy the normal weights attention weights to the global attention layers too\n",
    "        roberta_target_key = \".\".join([prefix, layer_idx, layer_class, layer_type, target.removesuffix(\"_global\"), params])\n",
    "        roberta_weights = roberta_parameters[roberta_target_key]\n",
    "        orig_weights = longformer_parameters[longformer_key]\n",
    "        longformer_parameters[longformer_key] = roberta_weights\n",
    "    \n",
    "    # Update the state of the longformer model\n",
    "    longformer_model.encoder.load_state_dict(longformer_parameters, strict=True)\n",
    "    \n",
    "    #------------#\n",
    "    # Embeddings #\n",
    "    #------------#\n",
    "    # There are two types of embeddings:\n",
    "    # 1. Token embeddings\n",
    "    # 2. Positional embeddings\n",
    "    # But we only need to copy the token embeddings \n",
    "    # while keeping the positional embeddings fixed.\n",
    "\n",
    "    roberta_embeddings_parameters = roberta_model.embeddings.state_dict()\n",
    "    embedding_parameters2copy = []\n",
    "    # We have to resize the token embeddings upfront, to make load_state_dict work.\n",
    "    longformer_model.resize_token_embeddings(len(roberta_tokenizer))\n",
    "    for key, item in roberta_embeddings_parameters.items():\n",
    "        if not \"position\" in key:\n",
    "            embedding_parameters2copy.append((key, item))\n",
    "    embedding_parameters2copy = OrderedDict(embedding_parameters2copy)\n",
    "    \n",
    "    longformer_model.embeddings.load_state_dict(embedding_parameters2copy, strict=False)\n",
    "\n",
    "    \n",
    "    return longformer_model, longformer_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d0acd3f-7501-42f1-9f71-7fac4e41b237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'LongformerTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "longformer_model, longformer_tokenizer = convert_roberta_to_longformer(\n",
    "    roberta_model=roberta_model,\n",
    "    roberta_tokenizer=roberta_tokenizer,\n",
    "    longformer_model=longformer_model,\n",
    "    longformer_max_length=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df57f697-7c2c-4301-ba12-20829e3761a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (85 > 12). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "inputs = longformer_tokenizer(\"Er sah eine irdische Zentralregierung, und er erblickte Frieden, Wohlstand und galaktische Anerkennung.\"\n",
    "                              \"Es war eine Vision, doch er nahm sie mit vollen Sinnen in sich auf.\"\n",
    "                              \"Im Laderaum der STARDUST begann eine rätselhafte Maschine zu summen.\"\n",
    "                              \"Die dritte Macht nahm die Arbeit auf.\"\n",
    "                              \"Da lächelte Perry Rhodan zum blauen Himmel empor.\"\n",
    "                              \"Langsam löste er die Rangabzeichen von dem Schulterstück seiner Kombination.\",\n",
    "                              return_tensors=\"pt\")\n",
    "ouputs = longformer_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac25c08f-d205-45f4-b8b5-91cd90e4eb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('layer.0.attention.self.query.weight',\n",
       " tensor([[-0.0269,  0.0167,  0.1127,  ...,  0.0181, -0.0102,  0.0051],\n",
       "         [ 0.0810,  0.0226,  0.0244,  ..., -0.0358,  0.0325,  0.0302],\n",
       "         [ 0.0144,  0.0280,  0.0135,  ..., -0.0652,  0.0628, -0.0140],\n",
       "         ...,\n",
       "         [-0.0131,  0.0109,  0.0334,  ..., -0.0476,  0.0423,  0.0392],\n",
       "         [ 0.0170,  0.0442, -0.0579,  ..., -0.0340, -0.0264, -0.0402],\n",
       "         [ 0.0092, -0.0088, -0.0338,  ...,  0.0180,  0.0569,  0.0584]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(longformer_model.encoder.state_dict().items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f9167d-2c99-4979-9c51-7f436949dea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('layer.0.attention.self.query.weight',\n",
       " tensor([[-0.0269,  0.0167,  0.1127,  ...,  0.0181, -0.0102,  0.0051],\n",
       "         [ 0.0810,  0.0226,  0.0244,  ..., -0.0358,  0.0325,  0.0302],\n",
       "         [ 0.0144,  0.0280,  0.0135,  ..., -0.0652,  0.0628, -0.0140],\n",
       "         ...,\n",
       "         [-0.0131,  0.0109,  0.0334,  ..., -0.0476,  0.0423,  0.0392],\n",
       "         [ 0.0170,  0.0442, -0.0579,  ..., -0.0340, -0.0264, -0.0402],\n",
       "         [ 0.0092, -0.0088, -0.0338,  ...,  0.0180,  0.0569,  0.0584]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(roberta_model.encoder.state_dict().items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d337e08-92c2-4053-8c76-cb7fd64a655c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('word_embeddings.weight',\n",
       " tensor([[-0.0071, -0.1073,  0.1197,  ..., -0.0265,  0.1016, -0.0173],\n",
       "         [-0.0232, -0.0226,  0.0150,  ..., -0.0031,  0.0157, -0.0189],\n",
       "         [ 0.0167,  0.2000,  0.0245,  ...,  0.0293, -0.0021,  0.0197],\n",
       "         ...,\n",
       "         [-0.0061, -0.0659,  0.0326,  ...,  0.0021,  0.0040, -0.0293],\n",
       "         [-0.0052,  0.0323,  0.0685,  ...,  0.0269,  0.0426, -0.0095],\n",
       "         [ 0.0011, -0.0304, -0.0013,  ...,  0.0030,  0.0210,  0.0010]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(longformer_model.embeddings.state_dict().items())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12a8d684-d7a6-47c9-902c-dfd6a59b1dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('word_embeddings.weight',\n",
       " tensor([[-0.0071, -0.1073,  0.1197,  ..., -0.0265,  0.1016, -0.0173],\n",
       "         [-0.0232, -0.0226,  0.0150,  ..., -0.0031,  0.0157, -0.0189],\n",
       "         [ 0.0167,  0.2000,  0.0245,  ...,  0.0293, -0.0021,  0.0197],\n",
       "         ...,\n",
       "         [-0.0061, -0.0659,  0.0326,  ...,  0.0021,  0.0040, -0.0293],\n",
       "         [-0.0052,  0.0323,  0.0685,  ...,  0.0269,  0.0426, -0.0095],\n",
       "         [ 0.0011, -0.0304, -0.0013,  ...,  0.0030,  0.0210,  0.0010]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(roberta_model.embeddings.state_dict().items())[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
